{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(f\"/home/{os.getlogin()}/watttime-python-client-aer-algo\")\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pytz\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import concurrent.futures\n",
    "\n",
    "from watttime import WattTimeForecast, WattTimeHistorical\n",
    "\n",
    "import optimizer.s3 as s3u\n",
    "import evaluation.eval_framework as efu\n",
    "\n",
    "username = os.getenv(\"WATTTIME_USER\")\n",
    "password = os.getenv(\"WATTTIME_PASSWORD\")\n",
    "\n",
    "actual_data = WattTimeHistorical(username, password)\n",
    "hist_data = WattTimeForecast(username, password)\n",
    "\n",
    "s3 = s3u.s3_utils()\n",
    "key = \"20240726_1k_synth_users_163_days.csv\"\n",
    "generated_data = s3.load_csvdataframe(file=key)\n",
    "generated_data = generated_data[-8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions = [\n",
    "# 'CAISO_NORTH',\n",
    "# 'SPP_TX',\n",
    "# 'ERCOT_EASTTX',\n",
    "# 'FPL',\n",
    "# 'SOCO',\n",
    "# 'PJM_CHICAGO',\n",
    "# 'LDWP',\n",
    "# 'PJM_DC',\n",
    "# 'NYISO_NYC'\n",
    "# ]\n",
    "regions = [\n",
    "    \"PJM_CHICAGO\",\n",
    "]\n",
    "region = regions[0]\n",
    "\n",
    "synth_data = generated_data.copy(deep=True)\n",
    "synth_data[\"session_start_time\"] = pd.to_datetime(synth_data[\"session_start_time\"])\n",
    "synth_data[\"unplug_time\"] = pd.to_datetime(synth_data[\"unplug_time\"])\n",
    "\n",
    "import pickle\n",
    "\n",
    "actual_pickle = s3.load_file(file=\"pjm_actual.pkl\")\n",
    "HISTORICAL_ACTUAL_CACHE = pickle.loads(actual_pickle)\n",
    "\n",
    "forecast_pickle = s3.load_file(file=\"pjm_forecast.pkl\")\n",
    "HISTORICAL_FORECAST_CACHE = pickle.loads(actual_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cached version of the get_*_data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def precache_actual_data(synth_data, regions):\n",
    "    distinct_dates = [\n",
    "        datetime.strptime(date, \"%Y-%m-%d\").date()\n",
    "        for date in synth_data[\"distinct_dates\"].unique().tolist()\n",
    "    ]\n",
    "    all_dates_regions = [\n",
    "        (date, region) for date in distinct_dates for region in regions\n",
    "    ]\n",
    "\n",
    "    def get_actual_data_for_region_date(date, region):\n",
    "        start = pd.to_datetime(date)\n",
    "        end = start + pd.Timedelta(\"2d\")\n",
    "        return (\n",
    "            region,\n",
    "            date,\n",
    "            actual_data.get_historical_pandas(\n",
    "                start - pd.Timedelta(\"9h\"),\n",
    "                end + pd.Timedelta(\"9h\"),\n",
    "                region,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        result = executor.map(\n",
    "            get_actual_data_for_region_date,\n",
    "            [date for (date, region) in all_dates_regions],\n",
    "            [region for (date, region) in all_dates_regions],\n",
    "        )\n",
    "    result = list(result)\n",
    "\n",
    "    return {(region, date): data for (region, date, data) in result}\n",
    "\n",
    "\n",
    "HISTORICAL_ACTUAL_CACHE = precache_actual_data(synth_data, regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def precache_fcst_data(synth_data, regions):\n",
    "    distinct_dates = [\n",
    "        datetime.strptime(date, \"%Y-%m-%d\").date()\n",
    "        for date in synth_data[\"distinct_dates\"].unique().tolist()\n",
    "    ]\n",
    "    all_dates_regions = [\n",
    "        (date, region) for date in distinct_dates for region in regions\n",
    "    ]\n",
    "\n",
    "    def get_fsct_data_for_region_date(date, region):\n",
    "        start = pd.to_datetime(date)\n",
    "        end = pd.to_datetime(date) + pd.Timedelta(\"1d\")\n",
    "        return (\n",
    "            region,\n",
    "            date,\n",
    "            hist_data.get_historical_forecast_pandas(\n",
    "                start - pd.Timedelta(\"9h\"),\n",
    "                end + pd.Timedelta(\"9h\"),\n",
    "                region,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        result = executor.map(\n",
    "            get_fsct_data_for_region_date,\n",
    "            [date for (date, region) in all_dates_regions],\n",
    "            [region for (date, region) in all_dates_regions],\n",
    "        )\n",
    "    result = list(result)\n",
    "    return {(region, date): data for (region, date, data) in result}\n",
    "\n",
    "\n",
    "HISTORICAL_FORECAST_CACHE = precache_fcst_data(synth_data, regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_fcst_data_cached(session_start_time, horizon, region):\n",
    "    time_zone = efu.get_timezone_from_dict(region)\n",
    "    session_start_time_utc = pd.Timestamp(\n",
    "        efu.convert_to_utc(session_start_time, time_zone)\n",
    "    )\n",
    "    date = session_start_time.date()\n",
    "    if (region, date) not in HISTORICAL_FORECAST_CACHE.keys():\n",
    "        print(type(date), date)\n",
    "        start = pd.to_datetime(date)\n",
    "        end = pd.to_datetime(date) + pd.Timedelta(\"1d\")\n",
    "        HISTORICAL_FORECAST_CACHE[(region, date)] = (\n",
    "            hist_data.get_historical_forecast_pandas(\n",
    "                start - pd.Timedelta(\"9h\"),\n",
    "                end + pd.Timedelta(\"9h\"),\n",
    "                region,\n",
    "            )\n",
    "        )\n",
    "    cache = HISTORICAL_FORECAST_CACHE[(region, date)]\n",
    "\n",
    "    # make this match efu.get_historical_fsct_data\n",
    "    generated_at_times = cache[\"generated_at\"].unique()\n",
    "    generated_at = max([t for t in generated_at_times if t < session_start_time_utc])\n",
    "    df = cache[cache[\"generated_at\"] == generated_at].copy()\n",
    "    return df.iloc[: math.ceil(horizon / 12) * 12]\n",
    "\n",
    "\n",
    "def get_historical_actual_data_cached(session_start_time, horizon, region):\n",
    "    time_zone = efu.get_timezone_from_dict(region)\n",
    "    session_start_time_utc = pd.Timestamp(\n",
    "        efu.convert_to_utc(session_start_time, time_zone)\n",
    "    )\n",
    "    date = session_start_time.date()\n",
    "\n",
    "    if (region, date) not in HISTORICAL_ACTUAL_CACHE.keys():\n",
    "        start = pd.to_datetime(date)\n",
    "        end = pd.to_datetime(date) + pd.Timedelta(\"2d\")\n",
    "        HISTORICAL_ACTUAL_CACHE[(region, date)] = actual_data.get_historical_pandas(\n",
    "            start - pd.Timedelta(\"9h\"),\n",
    "            end + pd.Timedelta(\"9h\"),\n",
    "            region,\n",
    "        )\n",
    "    cache = HISTORICAL_ACTUAL_CACHE[(region, date)]\n",
    "\n",
    "    t_start = max(\n",
    "        [t for t in cache[\"point_time\"].unique() if t < session_start_timet_time_utc]\n",
    "    )\n",
    "    df = cache[cache[\"point_time\"] >= t_start].copy()\n",
    "    return df.iloc[: math.ceil(horizon / 12) * 12 + 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Data with caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "synth_data[\"moer_data\"] = synth_data.apply(\n",
    "    lambda x: get_historical_fcst_data_cached(\n",
    "        x.session_start_time, math.ceil(x.total_intervals_plugged_in), region=region\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "synth_data['moer_data_actual'] = synth_data.apply(\n",
    "    lambda x: get_historical_actual_data_cached(\n",
    "    x.session_start_time,\n",
    "    math.ceil(x.total_intervals_plugged_in),\n",
    "    region = region\n",
    "    ), axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOER - No Optimization - Actual Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "get_charging_schedule_lambda = lambda x: x[\"usage\"].values.flatten()\n",
    "get_total_emissions_lambda = lambda x: x[\"emissions_co2e_lb\"].sum()\n",
    "\n",
    "synth_data[\"charger_baseline_actual_api\"] = synth_data.apply(\n",
    "    lambda x: efu.get_schedule_and_cost_api(\n",
    "        x.power_output_rate,\n",
    "        math.ceil(min(x.total_seconds_to_95, x.length_of_session_in_seconds) / 300.0)\n",
    "        * 5.0,\n",
    "        x.moer_data_actual,\n",
    "        optimization_method=\"baseline\",\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "synth_data[\"baseline_charging_schedule_api\"] = synth_data[\n",
    "    \"charger_baseline_actual_api\"\n",
    "].apply(get_charging_schedule_lambda)\n",
    "synth_data[\"baseline_actual_emissions_api\"] = synth_data[\n",
    "    \"charger_baseline_actual_api\"\n",
    "].apply(get_total_emissions_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOER - Simple Optimization - Forecast Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# TODO: I feel like this slicing might lead to bugs in the future\n",
    "def get_total_emission(moer, schedule):\n",
    "    x = np.array(schedule).flatten()\n",
    "    return np.dot(moer[: x.shape[0]], x)\n",
    "\n",
    "\n",
    "synth_data[\"charger_simple_forecast\"] = synth_data.apply(\n",
    "    lambda x: efu.get_schedule_and_cost_api(\n",
    "        x.power_output_rate,\n",
    "        int(\n",
    "            math.ceil(min(x.total_seconds_to_95, x.get_schedule_and_cost_api) / 300.0)\n",
    "            * 5\n",
    "        ),\n",
    "        x.moer_data,\n",
    "        optimization_method=\"simple\",\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "synth_data[\"simple_charging_schedule\"] = synth_data[\"charger_simple_forecast\"].apply(\n",
    "    get_charging_schedule_lambda\n",
    ")\n",
    "synth_data[\"simple_estimated_emissions\"] = synth_data[\"charger_simple_forecast\"].apply(\n",
    "    get_total_emissions_lambda\n",
    ")\n",
    "synth_data[\"simple_actual_emissions\"] = synth_data.apply(\n",
    "    lambda x: get_total_emission(\n",
    "        x.moer_data_actual[\"value\"],\n",
    "        x.charger_simple_forecast.energy_usage_mwh,\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "synth_data[\"charger_simple_actual\"] = synth_data.apply(\n",
    "    lambda x: efu.get_schedule_and_cost_api(\n",
    "        x.power_output_rate,\n",
    "        int(\n",
    "            math.ceil(\n",
    "                min(x.total_seconds_to_95, x.length_of_session_in_seconds) / 300.0\n",
    "            )\n",
    "            * 5\n",
    "        ),\n",
    "        x.moer_data_actual,\n",
    "        optimization_method=\"simple\",\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "synth_data[\"simple_actual_charging_schedule\"] = synth_data[\n",
    "    \"charger_simple_actual\"\n",
    "].apply(get_charging_schedule_lambda)\n",
    "synth_data[\"simple_ideal_emissions\"] = synth_data[\"charger_simple_actual\"].apply(\n",
    "    get_total_emissions_lambda\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer.s3 import s3_utils\n",
    "\n",
    "s3 = s3_utils()\n",
    "cols = [\n",
    "    \"user_type\",\n",
    "    \"power_output_rate\",\n",
    "    \"distinct_dates\",\n",
    "    \"session_start_time\",\n",
    "    \"total_intervals_plugged_in\",\n",
    "    \"charged_kWh_actual\",\n",
    "    \"MWh_fraction\",\n",
    "    \"simple_actual_emissions\",\n",
    "    \"baseline_actual_emissions\",\n",
    "    \"simple_estimated_emissions\",\n",
    "    \"simple_ideal_emissions\",\n",
    "]\n",
    "\n",
    "s3.store_csvdataframe(\n",
    "    synth_data[cols], f\"results_v2/20240726_1k_synth_users_163_days_{region}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
